{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "cSpEUK-8rjK5",
        "VDf5e3o1rzR6",
        "RfZ2lxyCr-Mn",
        "yaW3NijqsJ4k",
        "_yOgZTpnKbuq",
        "k6mjqpzgQiv5",
        "WgFdDy1ZdlTR",
        "7P_VsjySYsGM",
        "e910b849",
        "9fcd2696",
        "b3-zUHeQsexJ",
        "GzuES43nshBE"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Utility Functions"
      ],
      "metadata": {
        "id": "LcnxsicQrPjt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EMA Buffer"
      ],
      "metadata": {
        "id": "cSpEUK-8rjK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "EMA_DECAY = 0.99\n",
        "\n",
        "class LatentBuffer:\n",
        "    def __init__(self, embed_dim, ema_decay=EMA_DECAY):\n",
        "        self.prev_z_c = None\n",
        "        self.ema_decay = ema_decay\n",
        "\n",
        "    def store_prev_z_c(self, z_c_pooled):\n",
        "        self.prev_z_c = z_c_pooled.detach().clone()\n",
        "\n",
        "    def get_prev_z_c(self):\n",
        "        return self.prev_z_c\n",
        "\n",
        "def init_target_from_online(online, target):\n",
        "    target.load_state_dict(online.state_dict())\n",
        "    for p in target.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "def ema_update(online, target, decay=EMA_DECAY):\n",
        "    with torch.no_grad():\n",
        "        for p_o, p_t in zip(online.parameters(), target.parameters()):\n",
        "            p_t.data.mul_(decay).add_(p_o.data * (1 - decay))\n"
      ],
      "metadata": {
        "id": "wt75m51RryFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss Functions"
      ],
      "metadata": {
        "id": "VDf5e3o1rzR6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\"\"\"\n",
        "Zhu’s AD-L-JEPA keeps only the essentials for predictive BEV learning:\n",
        "\n",
        "Kept:\n",
        "  - JEPA masked embedding prediction (predict embeddings, not points)\n",
        "  - VICReg variance term only (simple + effective collapse prevention)\n",
        "  - EMA target encoder (stabilizes learning; replaces invariance)\n",
        "  - BEV-grid masking (empty + non-empty)\n",
        "\n",
        "Dropped:\n",
        "  - Invariance term (unnecessary: JEPA already aligns context→target)\n",
        "  - Covariance term (redundant + expensive; EMA + variance suffice)\n",
        "  - Contrastive negatives (not needed in predictive JEPA)\n",
        "  - Pixel/point-cloud reconstruction (not needed in embedding JEPA)\n",
        "\n",
        "When to use FULL VICReg (invariance + variance + covariance):\n",
        "  - Dual-view Siamese SSL (two augmentations to align)\n",
        "  - Multi-modal alignment (image↔text, audio↔video)\n",
        "  - No EMA or predictor used → full VICReg needed to avoid collapse\n",
        "  - When whitening / decorrelation improves downstream tasks\n",
        "\n",
        "When to use HALF VICReg (variance term only):\n",
        "  - JEPA-style predictive models (masked tokens, future embeddings)\n",
        "  - Architectures with EMA teacher → invariance unnecessary\n",
        "  - BEV / spatial-grid encoders where covariance is costly + low benefit\n",
        "  - Large-scale masking pretraining where efficiency matters\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def jepa_loss(\n",
        "    s_c,          # predicted embedding (B, N, D)\n",
        "    s_t,          # target embedding (B, N, D)\n",
        "    z_c,          # context encoder embedding AFTER empty/mask token replace (B,N,D)\n",
        "    mask_empty,   # (B, N)   masked empty grids (P)\n",
        "    mask_nonempty,# (B, N)   masked non-empty grids (Q)\n",
        "    alpha0=0.25,\n",
        "    alpha1=0.75,\n",
        "    beta1=1.0,\n",
        "    beta2=1.0,\n",
        "    lambda_jepa=1.0,\n",
        "    lambda_reg=1.0,\n",
        "    gamma=1.0\n",
        "):\n",
        "    B, N, D = s_c.shape\n",
        "    device = s_c.device\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 1. JEPA cosine loss on masked indices\n",
        "    # ---------------------------------------------------------\n",
        "\n",
        "    # -- EMPTY masked (P set)\n",
        "    mask_P = mask_empty.bool()                        # (B,N)\n",
        "    if mask_P.any():\n",
        "        s_c_P = s_c[mask_P].view(B, -1, D)\n",
        "        s_t_P = s_t[mask_P].view(B, -1, D)\n",
        "        cos_P = F.cosine_similarity(s_c_P, s_t_P, dim=-1)\n",
        "        loss_P = (1 - cos_P).mean()\n",
        "    else:\n",
        "        loss_P = torch.tensor(0.0, device=device)\n",
        "\n",
        "    # -- NON-EMPTY masked (Q set)\n",
        "    mask_Q = mask_nonempty.bool()\n",
        "    if mask_Q.any():\n",
        "        s_c_Q = s_c[mask_Q]        # (Kq, D)\n",
        "        s_t_Q = s_t[mask_Q]        # (Kq, D)\n",
        "        cos_Q = F.cosine_similarity(s_c_Q, s_t_Q, dim=-1)\n",
        "        loss_Q = (1 - cos_Q).mean()\n",
        "    else:\n",
        "        loss_Q = torch.tensor(0.0, device=device)\n",
        "\n",
        "    L_jepa = alpha0 * loss_P + alpha1 * loss_Q\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 2. Variance Regularization (VICReg-style)\n",
        "    # Only on non-empty masked grids K = Q\n",
        "    # ---------------------------------------------------------\n",
        "\n",
        "    L_reg = torch.tensor(0.0, device=device)\n",
        "\n",
        "    for b in range(B):\n",
        "        idx = mask_Q[b]   # indices of non-empty masked grids in sample b\n",
        "\n",
        "        if idx.any():\n",
        "            # Context embeddings z_c[K]\n",
        "            zc_K = z_c[b][idx]     # (M, D)\n",
        "            # Predictor embeddings s_c[Q]\n",
        "            sc_Q = s_c[b][idx]     # (M, D)\n",
        "\n",
        "            vr1 = variance_regularization(zc_K, gamma=gamma)\n",
        "            vr2 = variance_regularization(sc_Q, gamma=gamma)\n",
        "\n",
        "            L_reg += beta1 * vr1 + beta2 * vr2\n",
        "\n",
        "    L_reg = L_reg / B       # important: average per sample (per paper)\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 3. Total JEPA loss\n",
        "    # ---------------------------------------------------------\n",
        "    loss = lambda_jepa * L_jepa + lambda_reg * L_reg\n",
        "\n",
        "    return {\n",
        "        \"loss_total\": loss,\n",
        "        \"loss_jepa\": L_jepa,\n",
        "        \"loss_reg\": L_reg,\n",
        "        \"loss_P_empty\": loss_P,\n",
        "        \"loss_Q_nonempty\": loss_Q\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "def variance_regularization(z, gamma=1.0, eps=1e-4):\n",
        "    \"\"\"\n",
        "    z: (M, D)\n",
        "    Computes per-dimension variance hinge loss.\n",
        "    \"\"\"\n",
        "    if z.numel() == 0:\n",
        "        return torch.tensor(0.0, device=z.device)\n",
        "\n",
        "    std = torch.sqrt(z.var(dim=0) + eps)  # (D,)\n",
        "    return torch.mean(F.relu(gamma - std))\n",
        "\n",
        "\n",
        "def drift_loss(z_c, prev_z_c):\n",
        "    if prev_z_c is None:\n",
        "        return torch.tensor(0.0, device=z_c.device)\n",
        "    return F.mse_loss(z_c, prev_z_c)\n",
        "\n",
        "\n",
        "def vicreg_loss(z1, z2, sim_coeff=25.0, var_coeff=25.0, cov_coeff=1.0):\n",
        "    sim_loss = F.mse_loss(z1, z2)\n",
        "    eps = 1e-4\n",
        "    std_z1 = torch.sqrt(z1.var(dim=0) + eps)\n",
        "    std_z2 = torch.sqrt(z2.var(dim=0) + eps)\n",
        "    var_loss = torch.mean(F.relu(1.0 - std_z1)) + torch.mean(F.relu(1.0 - std_z2))\n",
        "    z1_centered = z1 - z1.mean(dim=0)\n",
        "    z2_centered = z2 - z2.mean(dim=0)\n",
        "    N, D = z1.shape\n",
        "    cov_z1 = (z1_centered.T @ z1_centered) / (N - 1)\n",
        "    cov_z2 = (z2_centered.T @ z2_centered) / (N - 1)\n",
        "    cov_z1.fill_diagonal_(0)\n",
        "    cov_z2.fill_diagonal_(0)\n",
        "    cov_loss = cov_z1.pow(2).sum() / D + cov_z2.pow(2).sum() / D\n",
        "    return sim_coeff * sim_loss + var_coeff * var_loss + cov_coeff * cov_loss\n",
        "\n"
      ],
      "metadata": {
        "id": "zQmCUU8xr2lW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Masking (Testing)"
      ],
      "metadata": {
        "id": "s2xVwtd7r5Lh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RGB → BEV semantic occupancy → patchify → mask → expand → apply mask to encoder tokens"
      ],
      "metadata": {
        "id": "8BWWMJVX_P1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_bev_mask_grid_occupancy(bev_rgb, mask_ratio=0.5, patch_size=16):\n",
        "    \"\"\"\n",
        "    BEV Masking using STRICT semantic occupancy:\n",
        "    EMPTY   = pixel == (50,50,50)\n",
        "    NON-EMPTY = anything not equal to gray road\n",
        "    \"\"\"\n",
        "    device = bev_rgb.device\n",
        "    B, C, H, W = bev_rgb.shape\n",
        "\n",
        "    # -----------------------------\n",
        "    # 1. Strict occupancy detection\n",
        "    # -----------------------------\n",
        "    background_gray = torch.tensor([50,50,50], device=device).view(1,3,1,1)\n",
        "    is_road = (bev_rgb == background_gray).all(dim=1)       # (B,H,W)\n",
        "    occupancy = (~is_road).float().unsqueeze(1)             # (B,1,H,W)\n",
        "\n",
        "    # -----------------------------\n",
        "    # 2. Patchify occupancy\n",
        "    # -----------------------------\n",
        "    tokens, ph, pw = patchify(occupancy, patch_size)\n",
        "    N = ph * pw\n",
        "    tokens = tokens.view(B, N, 1, patch_size, patch_size)\n",
        "\n",
        "    # patch non-empty = if any pixel in patch has occupancy\n",
        "    patch_occ = tokens.sum(dim=(2,3,4))   # (B,N)\n",
        "    patch_empty = patch_occ == 0\n",
        "    patch_nonempty = ~patch_empty\n",
        "\n",
        "    # -----------------------------\n",
        "    # 3. Random P/Q masking\n",
        "    # -----------------------------\n",
        "    mask_empty = torch.zeros(B, N, dtype=torch.bool, device=device)\n",
        "    mask_nonempty = torch.zeros(B, N, dtype=torch.bool, device=device)\n",
        "\n",
        "    num_mask_total = int(mask_ratio * N)\n",
        "\n",
        "    for b in range(B):\n",
        "        empty_idx = patch_empty[b].nonzero(as_tuple=False).squeeze(-1)\n",
        "        nonempty_idx = patch_nonempty[b].nonzero(as_tuple=False).squeeze(-1)\n",
        "\n",
        "        num_P = min(len(empty_idx), num_mask_total // 2)\n",
        "        num_Q = min(len(nonempty_idx), num_mask_total - num_P)\n",
        "\n",
        "        if num_P > 0:\n",
        "            mask_empty[b, empty_idx[torch.randperm(len(empty_idx))[:num_P]]] = True\n",
        "\n",
        "        if num_Q > 0:\n",
        "            mask_nonempty[b, nonempty_idx[torch.randperm(len(nonempty_idx))[:num_Q]]] = True\n",
        "\n",
        "    mask_any = mask_empty | mask_nonempty\n",
        "\n",
        "    # -----------------------------\n",
        "    # 4. Upsample to pixel mask\n",
        "    # -----------------------------\n",
        "    mask_grid = mask_any.view(B, 1, ph, pw).float()\n",
        "    mask_pixel = F.interpolate(mask_grid, size=(H, W), mode=\"nearest\").bool()\n",
        "\n",
        "    return mask_empty, mask_nonempty, mask_any, mask_pixel, ph, pw\n",
        "\n"
      ],
      "metadata": {
        "id": "6e-XXuDor3cT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "PATCH_SIZE = 16\n",
        "\n",
        "def create_non_empty_mask(\n",
        "        imgfile: str,\n",
        "        patch_size=PATCH_SIZE,\n",
        "        mask_ratio_nonempty=0.5,   # % of non-empty patches to mask\n",
        "        mask_ratio_empty=0.0,      # % of empty patches to mask (optional)\n",
        "        is_visualize=False\n",
        "    ):\n",
        "    # -------------------------------------------------------------\n",
        "    # 1. Analyze colors\n",
        "    # -------------------------------------------------------------\n",
        "    color_analysis = analyze_bev_colors(imgfile, top_k=100)\n",
        "\n",
        "    img = Image.open(imgfile).convert(\"RGB\")\n",
        "    arr = np.array(img)\n",
        "\n",
        "    # remove background gray\n",
        "    target_colors = [code for code, _ in color_analysis if code != [50, 50, 50]]\n",
        "\n",
        "    # -------------------------------------------------------------\n",
        "    # 2. Point sampling\n",
        "    # -------------------------------------------------------------\n",
        "    all_coords = []\n",
        "\n",
        "    for r, g, b in target_colors:\n",
        "        ys, xs = np.where(\n",
        "            (arr[:, :, 0] == r) &\n",
        "            (arr[:, :, 1] == g) &\n",
        "            (arr[:, :, 2] == b)\n",
        "        )\n",
        "\n",
        "        if len(xs) == 0:\n",
        "            continue\n",
        "\n",
        "        idx = np.random.choice(len(xs), size=min(100, len(xs)), replace=False)\n",
        "        xs_s = xs[idx]\n",
        "        ys_s = ys[idx]\n",
        "\n",
        "        coords = np.vstack([xs_s, ys_s]).T\n",
        "        all_coords.append(coords)\n",
        "\n",
        "    if len(all_coords) == 0:\n",
        "        return None\n",
        "\n",
        "    all_coords = np.vstack(all_coords)\n",
        "\n",
        "    # -------------------------------------------------------------\n",
        "    # 3. Spatial hierarchical clustering\n",
        "    # -------------------------------------------------------------\n",
        "    model = AgglomerativeClustering(\n",
        "        n_clusters=None,\n",
        "        distance_threshold=40,\n",
        "        linkage=\"ward\"\n",
        "    )\n",
        "    labels = model.fit_predict(all_coords)\n",
        "    clusters = [all_coords[labels == lbl] for lbl in np.unique(labels)]\n",
        "\n",
        "    # -------------------------------------------------------------\n",
        "    # 4. Create pixel-level circular mask\n",
        "    # -------------------------------------------------------------\n",
        "    H, W = arr.shape[:2]\n",
        "    mask_pixel = np.zeros((H, W), dtype=np.uint8)\n",
        "\n",
        "    for pts in clusters:\n",
        "        xs, ys = pts[:, 0], pts[:, 1]\n",
        "        cx, cy = int(xs.mean()), int(ys.mean())\n",
        "\n",
        "        dists = np.sqrt((xs - cx)**2 + (ys - cy)**2)\n",
        "        radius = int(dists.max() * 1.25)\n",
        "\n",
        "        yy, xx = np.ogrid[:H, :W]\n",
        "        circle_mask = (xx - cx)**2 + (yy - cy)**2 <= radius**2\n",
        "        mask_pixel[circle_mask] = 1\n",
        "\n",
        "    # =============================================================\n",
        "    # 5. Patchify version\n",
        "    # =============================================================\n",
        "    mask_torch = torch.tensor(mask_pixel, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "    tokens, ph, pw = patchify(mask_torch, patch_size)\n",
        "    patch_sums = tokens.sum(dim=-1)          # (1, N)\n",
        "    patch_nonempty = (patch_sums > 0)        # (1, N)\n",
        "    patch_empty = ~patch_nonempty            # (1, N)\n",
        "\n",
        "    # =============================================================\n",
        "    # 6. Random Sampling Masking (NEW — JEPA-style partial masking)\n",
        "    # =============================================================\n",
        "    B, N = patch_nonempty.shape\n",
        "\n",
        "    # --- Non-empty patches (object regions)\n",
        "    K_idx = torch.where(patch_nonempty[0])[0]\n",
        "    num_K_mask = int(mask_ratio_nonempty * len(K_idx))\n",
        "    perm_K = torch.randperm(len(K_idx))\n",
        "    K_mask_idx = K_idx[perm_K[:num_K_mask]]\n",
        "\n",
        "    # --- Empty patches (optional)\n",
        "    E_idx = torch.where(patch_empty[0])[0]\n",
        "    num_E_mask = int(mask_ratio_empty * len(E_idx))\n",
        "    perm_E = torch.randperm(len(E_idx))\n",
        "    E_mask_idx = E_idx[perm_E[:num_E_mask]]\n",
        "\n",
        "    # --- Final mask over patches\n",
        "    patch_mask = torch.zeros_like(patch_nonempty)\n",
        "    patch_mask[0, K_mask_idx] = 1\n",
        "    patch_mask[0, E_mask_idx] = 1\n",
        "\n",
        "    # Expand for unpatchify\n",
        "    token_dim = patch_size * patch_size\n",
        "    patch_mask_tokens = patch_mask.float().unsqueeze(-1).repeat(1, 1, token_dim)\n",
        "\n",
        "    mask_pixel_restored = unpatchify(patch_mask_tokens, ph, pw, patch_size)\n",
        "    mask_pixel_restored = mask_pixel_restored[0,0].cpu().numpy().astype(np.uint8)\n",
        "\n",
        "    # -------------------------------------------------------------\n",
        "    # 7. Visualization\n",
        "    # -------------------------------------------------------------\n",
        "    if is_visualize:\n",
        "        plt.figure(figsize=(6, 6))\n",
        "        plt.imshow(arr)\n",
        "        plt.axis(\"off\")\n",
        "        plt.title(\"Cluster Circles (Pixel-Space)\")\n",
        "\n",
        "        for pts in clusters:\n",
        "            xs, ys = pts[:, 0], pts[:, 1]\n",
        "            cx, cy = int(xs.mean()), int(ys.mean())\n",
        "            radius = int(np.sqrt((xs - cx)**2 + (ys - cy)**2).max() * 1.25)\n",
        "            circ = plt.Circle((cx, cy), radius, edgecolor=\"cyan\", fill=False, linewidth=2, alpha=0.8)\n",
        "            plt.gca().add_patch(circ)\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "        plt.figure(figsize=(6, 6))\n",
        "        plt.imshow(mask_pixel_restored, cmap=\"gray\")\n",
        "        plt.title(f\"Partial Non-Empty Mask (Q-set, {mask_ratio_nonempty * 100}% \\n of obj been masked)\")\n",
        "        plt.colorbar()\n",
        "        plt.show()\n",
        "\n",
        "    # -------------------------------------------------------------\n",
        "    # 8. Return final outputs\n",
        "    # -------------------------------------------------------------\n",
        "    return {\n",
        "        \"mask_pixel\": mask_pixel,\n",
        "        \"patch_nonempty\": patch_nonempty.cpu(),\n",
        "        \"patch_mask\": patch_mask.cpu(),\n",
        "        \"mask_pixel_restored\": mask_pixel_restored,\n",
        "        \"ph\": ph,\n",
        "        \"pw\": pw\n",
        "    }"
      ],
      "metadata": {
        "id": "uh3CC8e49YYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLPEmbdedding predictor"
      ],
      "metadata": {
        "id": "RfZ2lxyCr-Mn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MLPPredictor(nn.Module):\n",
        "    def __init__(self, dim_s, dim_latent, hidden_dim, out_dim):\n",
        "        \"\"\"\n",
        "        :param dim_s       : dimension of the first input (s_c)\n",
        "        :param dim_latent  : dimension of the second input (z_latent)\n",
        "        :param hidden_dim  : hidden layer size\n",
        "        :param out_dim     : output dimension (e.g., embedding dim you predict)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.input_dim = dim_s + dim_latent\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(self.input_dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_dim, out_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, s_c, z_latent):\n",
        "        \"\"\"\n",
        "        :param s_c       : tensor of shape (B, dim_s)\n",
        "        :param z_latent  : tensor of shape (B, dim_latent)\n",
        "        \"\"\"\n",
        "        # concatenate along feature dimension\n",
        "        x = torch.cat((s_c, z_latent), dim=-1)  # shape (B, dim_s + dim_latent)\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "4G-BPu5esCUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Config"
      ],
      "metadata": {
        "id": "yaW3NijqsJ4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "EMBED_DIM = 128\n",
        "PATCH_SIZE = 16\n",
        "IMAGE_H = 32\n",
        "IMAGE_W = 32\n",
        "TOKEN_DIM = 3 * PATCH_SIZE * PATCH_SIZE\n",
        "ACTION_DIM = 4\n",
        "MASK_RATIO = 0.15\n",
        "VICREG_WEIGHT = 0.1\n",
        "DRIFT_WEIGHT = 0.05\n",
        "JEPA_WEIGHT = 1.0\n",
        "EMA_DECAY = 0.99\n",
        "BATCH_SIZE = 8\n",
        "NUM_STEPS = 50\n",
        "LR = 1e-3\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "DATA_ROOT = \"/kaggle/input/test1t/exported_maps\"\n"
      ],
      "metadata": {
        "id": "SA0ZXdnUsLTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## patch_util"
      ],
      "metadata": {
        "id": "iOmELlpmsDGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def patchify(imgs, patch_size=PATCH_SIZE):\n",
        "    \"\"\"\n",
        "    imgs: (B, C, H, W)\n",
        "    returns tokens: (B, N, token_dim), ph, pw\n",
        "    \"\"\"\n",
        "    B, C, H, W = imgs.shape\n",
        "    assert H % patch_size == 0 and W % patch_size == 0\n",
        "    ph = H // patch_size\n",
        "    pw = W // patch_size\n",
        "    x = imgs.reshape(B, C, ph, patch_size, pw, patch_size)\n",
        "    x = x.permute(0, 2, 4, 1, 3, 5).reshape(B, ph * pw, C * patch_size * patch_size)\n",
        "    return x, ph, pw\n",
        "\n",
        "def unpatchify(tokens, ph, pw, patch_size=PATCH_SIZE):\n",
        "    \"\"\"\n",
        "    tokens: (B, N, token_dim)\n",
        "    returns imgs: (B, C, H, W)\n",
        "    \"\"\"\n",
        "    B, N, token_dim = tokens.shape\n",
        "    C = token_dim // (patch_size * patch_size)\n",
        "    x = tokens.reshape(B, ph, pw, C, patch_size, patch_size)\n",
        "    x = x.permute(0, 3, 1, 4, 2, 5).reshape(B, C, ph * patch_size, pw * patch_size)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "MvFGcY7qsG3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test the Masking Function & Update with the color code analysis"
      ],
      "metadata": {
        "id": "_yOgZTpnKbuq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from PIL import Image\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# import numpy as np\n",
        "\n",
        "# # Load the user's BEV image\n",
        "# img = Image.open(\"map_0057.png\").convert(\"RGB\") # Take the\n",
        "# arr = np.array(img)  # shape (H, W, 3)\n",
        "\n",
        "# H, W, C = arr.shape # 512 x 512 x 3\n",
        "\n",
        "# bev = torch.tensor(arr).permute(2,0,1).unsqueeze(0).float()\n",
        "\n",
        "# # mask = 1 (WHITE), Not mask = 0 (DARK)\n",
        "# mask_emp, masked_nonempty, mask_any, _, ph, pw =create_bev_mask_grid_occupancy(bev, mask_ratio=0.5)\n",
        "\n",
        "# plt.figure(figsize=(6,6))\n",
        "# plt.imshow(mask_emp[0].view(ph,pw).cpu().numpy(), cmap='gray')\n",
        "# plt.title(\"Masked Empty (P-set)\")\n",
        "# plt.axis('off')\n",
        "\n",
        "# plt.figure(figsize=(6,6))\n",
        "# plt.imshow(masked_nonempty[0].view(ph,pw).cpu().numpy(), cmap='gray')\n",
        "# plt.title(\"Masked Non-Empty (Q-set)\")\n",
        "# plt.axis('off')\n",
        "\n",
        "# plt.figure(figsize=(6,6))\n",
        "# plt.imshow(mask_any[0].view(ph,pw).cpu().numpy(), cmap='gray')\n",
        "# plt.title(\"Masked Any  (P ∪ Q set)\")\n",
        "# plt.axis('off')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "YFi3G-hi2NqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "def analyze_bev_colors(image_path, top_k=100):\n",
        "    \"\"\"\n",
        "    Analyze the unique RGB colors in a BEV image and return\n",
        "    (color, count) sorted by frequency.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to the BEV image.\n",
        "        top_k (int): Number of most frequent colors to show.\n",
        "\n",
        "    Returns:\n",
        "        List of (RGB, count) tuples sorted by descending count.\n",
        "    \"\"\"\n",
        "    # Load image\n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "    arr = np.array(img)                       # (H,W,3)\n",
        "\n",
        "    # Flatten pixel array to shape (N,3)\n",
        "    pixels = arr.reshape(-1, 3)\n",
        "\n",
        "    # Unique colors + counts\n",
        "    unique_colors, counts = np.unique(pixels, axis=0, return_counts=True)\n",
        "\n",
        "    # Sort by count (descending)\n",
        "    sorted_idx = np.argsort(-counts)\n",
        "    unique_colors_sorted = unique_colors[sorted_idx]\n",
        "    counts_sorted = counts[sorted_idx]\n",
        "\n",
        "    # Return top colors\n",
        "    return [(unique_colors_sorted[i].tolist(), int(counts_sorted[i]))\n",
        "            for i in range(min(top_k, len(unique_colors_sorted)))]"
      ],
      "metadata": {
        "id": "cTE7cM7dNUia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing for the mask"
      ],
      "metadata": {
        "id": "k6mjqpzgQiv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from PIL import Image\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# results = analyze_bev_colors(\"map_0057.png\", top_k=100)\n",
        "\n",
        "# # Load image\n",
        "# img = Image.open(\"map_0057.png\").convert(\"RGB\")\n",
        "# arr = np.array(img)\n",
        "\n",
        "# # Colors of interest (from earlier analysis)\n",
        "# target_colors = [code for code, _ in results if code != [50, 50, 50]]\n",
        "# # target_colors = [code for code, _ in results ]\n",
        "\n",
        "\n",
        "# # Prepare figure\n",
        "# plt.figure(figsize=(8,8))\n",
        "# plt.imshow(arr)\n",
        "# plt.title(\"Color Locations in BEV\")\n",
        "# plt.axis(\"off\")\n",
        "\n",
        "# sampled_points = {}\n",
        "# # For each color, pick 30 random sample pixels and plot markers\n",
        "# for r,g,b in target_colors:\n",
        "#     mask = np.where(\n",
        "#         (arr[:,:,0]==r)&(arr[:,:,1]==g)&(arr[:,:,2]==b)\n",
        "#     )\n",
        "#     ys, xs = mask\n",
        "\n",
        "\n",
        "\n",
        "#     if len(xs) == 0:\n",
        "#         continue  # color doesn't exist\n",
        "\n",
        "#     # sample up to 100 points\n",
        "#     idx = np.random.choice(len(xs), size=min(100,len(xs)), replace=False)\n",
        "#     xs_s = xs[idx]\n",
        "#     ys_s = ys[idx]\n",
        "\n",
        "#     sampled_points[(r, g, b)] = {\n",
        "#         \"xs\": xs_s,\n",
        "#         \"ys\": ys_s\n",
        "#     }\n",
        "\n",
        "#     # plot on figure\n",
        "#     plt.scatter(xs_s, ys_s, s=20, label=f\"{(r,g,b)}\")\n",
        "\n",
        "# plt.legend(loc=\"upper left\")\n",
        "# plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "NYTmmu_wNUSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# all_coords = []\n",
        "\n",
        "# for color, pts in sampled_points.items():\n",
        "#     xs = pts[\"xs\"]\n",
        "#     ys = pts[\"ys\"]\n",
        "\n",
        "#     coords = np.vstack([xs, ys]).T\n",
        "#     all_coords.append(coords)\n",
        "\n",
        "# all_coords = np.vstack(all_coords)   # shape (N, 2)\n",
        "\n",
        "# from sklearn.cluster import AgglomerativeClustering\n",
        "# import numpy as np\n",
        "\n",
        "# model = AgglomerativeClustering(\n",
        "#     n_clusters=None,\n",
        "#     distance_threshold=40,   # tune this radius!\n",
        "#     linkage=\"ward\"\n",
        "# )\n",
        "\n",
        "# labels = model.fit_predict(all_coords)\n",
        "\n",
        "# clusters = []\n",
        "# for lbl in np.unique(labels):\n",
        "#     clusters.append(all_coords[labels == lbl])\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# plt.figure(figsize=(10,10))\n",
        "# plt.imshow(arr)\n",
        "# plt.axis(\"off\")\n",
        "\n",
        "# for pts in clusters:\n",
        "#     xs = pts[:,0]\n",
        "#     ys = pts[:,1]\n",
        "\n",
        "#     # centroid\n",
        "#     cx = xs.mean()\n",
        "#     cy = ys.mean()\n",
        "\n",
        "#     # approximate radius: average distance to centroid\n",
        "#     radius = np.mean(np.sqrt((xs - cx)**2 + (ys - cy)**2))\n",
        "\n",
        "#     circ = plt.Circle(\n",
        "#         (cx, cy),\n",
        "#         radius,\n",
        "#         edgecolor='cyan',\n",
        "#         linewidth=2,\n",
        "#         fill=False,\n",
        "#         alpha=0.8\n",
        "#     )\n",
        "#     plt.gca().add_patch(circ)\n",
        "\n",
        "# plt.show()\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "WV7WF_Qm6340"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create non-empty mask"
      ],
      "metadata": {
        "id": "XtPEcFT9QofN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Testing the non-empty masking"
      ],
      "metadata": {
        "id": "WgFdDy1ZdlTR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mask_emp.shape\n",
        "# mask = create_non_empty_mask(\"map_0057.png\")\n",
        "# mask[\"patch_nonempty\"].shape\n",
        "\n",
        "# print(mask_emp.shape)\n",
        "# print(mask[\"patch_nonempty\"].shape)\n",
        "\n",
        "# mask = create_non_empty_mask(\"map_0057.png\")\n",
        "\n",
        "# # Extract patch mask (1, 1024)\n",
        "# patch_nonempty = mask[\"patch_nonempty\"][0].cpu().numpy()   # (1024,)\n",
        "# mask_emp_np = mask_emp.cpu().numpy()[0]                    # (1024,)\n",
        "\n",
        "# # OR them\n",
        "# mask_union = patch_nonempty | mask_emp_np                  # (1024,)\n",
        "\n",
        "# # Convert to patch grid\n",
        "# ph, pw = mask[\"ph\"], mask[\"pw\"]     # both = 32 for 16×16 patches\n",
        "# mask_grid = mask_union.reshape(ph, pw)\n",
        "\n",
        "# plt.figure(figsize=(6,6))\n",
        "# plt.imshow(mask_grid, cmap='gray')\n",
        "# plt.title(\"Patch-Level Mask Union\")\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "hY-Wh2J1NBbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final masking function"
      ],
      "metadata": {
        "id": "7P_VsjySYsGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "def masking(image_file: str, visualize=False, empty_mask_ratio = 0.25):\n",
        "  # Load the user's BEV image\n",
        "  img = Image.open(image_file).convert(\"RGB\") # Take the\n",
        "  arr = np.array(img)  # shape (H, W, 3)\n",
        "\n",
        "  H, W, C = arr.shape # 512 x 512 x 3\n",
        "\n",
        "  bev = torch.tensor(arr).permute(2,0,1).unsqueeze(0).float()\n",
        "\n",
        "  # mask = 1 (WHITE), Not mask = 0 (DARK)\n",
        "  mask_emp, _, _, _, ph, pw =create_bev_mask_grid_occupancy(bev, mask_ratio=empty_mask_ratio) # (1024,)\n",
        "\n",
        "  mask_non_empty_dict = create_non_empty_mask(image_file, is_visualize=visualize )\n",
        "\n",
        "  if mask_non_empty_dict is None:\n",
        "    return None\n",
        "\n",
        "  mask_non_emp_np = mask_non_empty_dict[\"patch_nonempty\"][0].cpu().numpy()   # (1024,) - Q set\n",
        "  mask_emp_np = mask_emp.cpu().numpy()[0]                    # (1024,) - P set\n",
        "\n",
        "  # union of both P U Q\n",
        "  mask_union = mask_non_emp_np | mask_emp_np                  # (1024,)\n",
        "\n",
        "  # Convert to patch grid\n",
        "  ph, pw = mask_non_empty_dict[\"ph\"], mask_non_empty_dict[\"pw\"]     # the height & width for the patch - H / PATCH_SIZE = pH, W / PATCH_SIZE = pW\n",
        "  mask_grid = mask_union.reshape(ph, pw)\n",
        "\n",
        "\n",
        "\n",
        "  if visualize:\n",
        "    plt.figure(figsize=(6,6))\n",
        "    plt.imshow(mask_emp[0].view(ph,pw).cpu().numpy(), cmap='gray')\n",
        "    plt.title(f\"Masked Empty (P-Set) {empty_mask_ratio * 100} % of Empty Region \\n been masked\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.figure(figsize=(6,6))\n",
        "    plt.imshow(mask_grid, cmap='gray')\n",
        "    plt.title(\"Patch-Level Mask Union Non-Empty & Empty (P U Q)\")\n",
        "    plt.show()\n",
        "\n",
        "  transform = transforms.ToTensor()\n",
        "  img = img = transform(img)                # <---- IMPORTANT\n",
        "  return mask_emp_np, mask_non_emp_np, mask_union, ph, pw, bev, img\n"
      ],
      "metadata": {
        "id": "QrBEeQvYYwIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# masking(\"/content/dataset/exported_maps/maps/map_21597.png\", visualize=True)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "4h_AGToqdpzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "U = entire BEV grid\n",
        "│\n",
        "├── K = all non-empty (road + objects + curbs + paint)\n",
        "│      ├── R = road region (% take a percentent of that - left remain show)\n",
        "│      │     ├── O = object-like region (your clusters)\n",
        "│      │     └── R\\O = road surface with no detected object\n",
        "│      └── K\\R = other non-empty (sidewalk, vegetation, etc.)\n",
        "│\n",
        "└── E = all empty (background / padding / no LiDAR)\n",
        "       └── B = background region (your P samples usually come from here)\n",
        "\n",
        "Q ⊂ K   (masked non-empty)\n",
        "Q ⊂ R\n",
        "P ⊂ E   (masked empty)\n",
        "P ∩ Q = ∅\n",
        "P ∪ Q = masked subset (NOT whole scene)\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "fb_jA-mjnHGs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 9520
        },
        "collapsed": true,
        "outputId": "607ddbc1-8c51-456c-a7dd-b07fb14f952a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:7: SyntaxWarning: invalid escape sequence '\\O'\n",
            "<>:7: SyntaxWarning: invalid escape sequence '\\O'\n",
            "/tmp/ipython-input-1658913576.py:7: SyntaxWarning: invalid escape sequence '\\O'\n",
            "  │      │     └── R\\O = road surface with no detected object\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nU = entire BEV grid\\n│\\n├── K = all non-empty (road + objects + curbs + paint)\\n│      ├── R = road region (% take a percentent of that - left remain show)\\n│      │     ├── O = object-like region (your clusters)\\n│      │     └── R\\\\O = road surface with no detected object\\n│      └── K\\\\R = other non-empty (sidewalk, vegetation, etc.)\\n│\\n└── E = all empty (background / padding / no LiDAR)\\n       └── B = background region (your P samples usually come from here)\\n\\nQ ⊂ K   (masked non-empty)\\nQ ⊂ R\\nP ⊂ E   (masked empty)\\nP ∩ Q = ∅\\nP ∪ Q = masked subset (NOT whole scene)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Preparation"
      ],
      "metadata": {
        "id": "7o6WJECWh1x9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"min1124/a-crude-data-set-converted-from-nuscene\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9vYp6kIgSHS",
        "outputId": "47a23fd2-9927-4b5c-c8e0-00cbea19bb16",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/min1124/a-crude-data-set-converted-from-nuscene?dataset_version_number=2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 514M/514M [00:13<00:00, 41.1MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/min1124/a-crude-data-set-converted-from-nuscene/versions/2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /root/.cache/kagglehub/datasets/min1124/a-crude-data-set-converted-from-nuscene/versions/2 /content/dataset/"
      ],
      "metadata": {
        "id": "saGuqg20InLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "pkl_path = \"/content/dataset/exported_maps/graphs/graph_scene-0001.gpickle\"  # <-- change to your file path\n",
        "\n",
        "with open(pkl_path, \"rb\") as f:\n",
        "    G = pickle.load(f)\n",
        "\n",
        "print(type(G))\n",
        "print(\"num nodes:\", G.number_of_nodes())\n",
        "print(\"num edges:\", G.number_of_edges())\n",
        "\n",
        "# for n, data in G.nodes(data=True):\n",
        "#     print(n, data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yKpeIPz50AK",
        "outputId": "42aff3f6-f22e-4955-d5cf-10e493a1d351"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'networkx.classes.graph.Graph'>\n",
            "num nodes: 1730\n",
            "num edges: 15467\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77e84795"
      },
      "source": [
        "# Map-Dataset Preparation\n",
        "Create a CSV file named \"map_files.csv\" containing a list of all PNG filenames from the directory \"/content/dataset/exported_maps/maps\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e910b849"
      },
      "source": [
        "## List PNG Files\n",
        "\n",
        "### Subtask:\n",
        "List all files in the '/content/dataset/exported_maps/maps' directory and filter them to get only the PNG files.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f45e894"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires listing all PNG files in a specified directory. The previous cell already attempted to list CSV files, but the instruction is to list PNG files. I will now explicitly list the PNG files according to the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d17146a8",
        "outputId": "2e395570-5b11-427a-9810-f947fffc9a7d",
        "collapsed": true
      },
      "source": [
        "import os\n",
        "\n",
        "# 2. Define a variable, for example `map_directory`, and assign it the path '/content/dataset/exported_maps/maps'.\n",
        "map_directory = \"/content/dataset/exported_maps/maps\"\n",
        "\n",
        "# 3. Use `os.listdir()` to get a list of all files and directories within `map_directory`.\n",
        "all_files = os.listdir(map_directory)\n",
        "\n",
        "# 4. Create a new list called `png_files` by iterating through the list from the previous step and adding only those files that have a '.png' extension to this new list.\n",
        "png_files = [f for f in all_files if f.endswith('.png')]\n",
        "\n",
        "print(f\"Found {len(png_files)} PNG files in {map_directory}:\")\n",
        "# Print the first 10 PNG files as an example, if there are many\n",
        "for f in png_files[:10]:\n",
        "    print(f)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 31764 PNG files in /content/dataset/exported_maps/maps:\n",
            "map_13842.png\n",
            "map_29650.png\n",
            "map_27643.png\n",
            "map_30376.png\n",
            "map_18449.png\n",
            "map_0097.png\n",
            "map_23909.png\n",
            "map_10349.png\n",
            "map_20628.png\n",
            "map_25580.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27b29965"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the PNG files have been listed, the next step is to write these filenames into a CSV file named 'map_files.csv'. This will complete the main task.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0d2d098",
        "outputId": "c61c749d-b3af-4b57-c037-dfa375bcfa65"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a DataFrame from the list of PNG filenames\n",
        "df_map_files = pd.DataFrame(png_files, columns=['filename'])\n",
        "\n",
        "# Define the output CSV file path\n",
        "output_csv_path = \"map_files.csv\"\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df_map_files.to_csv(output_csv_path, index=False)\n",
        "\n",
        "print(f\"Successfully created '{output_csv_path}' with {len(png_files)} filenames.\")\n",
        "print(df_map_files.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully created 'map_files.csv' with 31764 filenames.\n",
            "        filename\n",
            "0  map_13842.png\n",
            "1  map_29650.png\n",
            "2  map_27643.png\n",
            "3  map_30376.png\n",
            "4  map_18449.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fcd2696"
      },
      "source": [
        "## Verify CSV Output\n",
        "\n",
        "### Subtask:\n",
        "Check if the CSV file was successfully created and display its content or a confirmation message.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd90e80c"
      },
      "source": [
        "**Reasoning**:\n",
        "To verify the CSV output, I need to load the generated 'map_files.csv' into a pandas DataFrame and display its first few rows to confirm that the filenames are correctly stored.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd310c51",
        "outputId": "5a667f17-9a9d-4095-84b3-b6ca874b423f"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1. Load the 'map_files.csv' file into a pandas DataFrame.\n",
        "output_csv_path = \"map_files.csv\"\n",
        "df_loaded_map_files = pd.read_csv(output_csv_path)\n",
        "\n",
        "# 2. Display the first few rows of the loaded DataFrame to confirm its content.\n",
        "print(f\"Successfully loaded '{output_csv_path}'. First 5 rows:\")\n",
        "print(df_loaded_map_files.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded 'map_files.csv'. First 5 rows:\n",
            "        filename\n",
            "0  map_13842.png\n",
            "1  map_29650.png\n",
            "2  map_27643.png\n",
            "3  map_30376.png\n",
            "4  map_18449.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6462c81"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Confirm the creation of the CSV file containing the list of PNG files.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d11b95c9"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "*   **Was the CSV file containing the list of PNG files successfully created and verified?**\n",
        "    Yes, a CSV file named `map_files.csv` was successfully created, containing a list of all PNG filenames from the specified directory. Its content was verified to be correct.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   A total of 31,764 PNG files were identified in the `/content/dataset/exported_maps/maps` directory.\n",
        "*   A CSV file named `map_files.csv` was successfully generated, containing all 31,764 PNG filenames.\n",
        "*   The `map_files.csv` file was loaded and verified, confirming it correctly listed the PNG filenames (e.g., `map_13485.png`, `map_0113.png`).\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The `map_files.csv` now serves as a comprehensive manifest of all PNG assets, which can be valuable for asset management or further data processing.\n",
        "*   The generated CSV file can be used as input for subsequent tasks, such as automated image processing, cataloging, or dataset creation, ensuring all relevant PNG files are included.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# JEPA-Tier-1"
      ],
      "metadata": {
        "id": "XlLfH8dBsX-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_mask(bev, mask_emp_np, mask_non_emp_np, mask_any_np, visualize = False):\n",
        "    # 1. Patchify BEV\n",
        "    tokens, ph, pw = patchify(bev, patch_size=PATCH_SIZE)\n",
        "\n",
        "    # 2. Convert mask to tensor\n",
        "    mask_any = torch.tensor(mask_any_np, dtype=torch.bool)\n",
        "    mask_emp = torch.tensor(mask_emp_np, dtype=torch.bool)\n",
        "    mask_non_emp = torch.tensor(mask_non_emp_np, dtype=torch.bool)\n",
        "\n",
        "\n",
        "    # 3. Apply mask in token space\n",
        "    tokens_masked_emp = tokens.clone()\n",
        "    tokens_masked_non_emp = tokens.clone()\n",
        "    token_masked_any = tokens.clone()\n",
        "\n",
        "    tokens_masked_emp[0, mask_emp] = 0\n",
        "    tokens_masked_non_emp[0, mask_non_emp] = 0\n",
        "    token_masked_any[0, mask_any] = 0\n",
        "\n",
        "\n",
        "    bev_masked_emp = unpatchify(tokens_masked_emp, ph, pw, patch_size=PATCH_SIZE)\n",
        "    bev_masked_non_emp  = unpatchify(tokens_masked_non_emp, ph, pw, patch_size=PATCH_SIZE)\n",
        "    bev_masked_any = unpatchify(token_masked_any, ph, pw, patch_size=PATCH_SIZE)\n",
        "\n",
        "    if visualize:\n",
        "      img_emp = bev_masked_emp[0].permute(1,2,0).cpu().numpy().astype(\"uint8\")\n",
        "      img_non_emp = bev_masked_non_emp[0].permute(1,2,0).cpu().numpy().astype(\"uint8\")\n",
        "      img_any = bev_masked_any[0].permute(1,2,0).cpu().numpy().astype(\"uint8\")\n",
        "\n",
        "      plt.figure(figsize=(6,6))\n",
        "      plt.imshow(img_emp)\n",
        "      plt.axis(\"off\")\n",
        "\n",
        "      plt.figure(figsize=(6,6))\n",
        "      plt.imshow(img_non_emp)\n",
        "      plt.axis(\"off\")\n",
        "\n",
        "\n",
        "      plt.figure(figsize=(6,6))\n",
        "      plt.imshow(img_any)\n",
        "      plt.axis(\"off\")\n",
        "\n",
        "    return bev_masked_emp, bev_masked_non_emp, bev_masked_any\n"
      ],
      "metadata": {
        "id": "OFVm1OtNIg01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class MapDataset(Dataset):\n",
        "\n",
        "    def __init__(self, map_csv_file: str):\n",
        "\n",
        "        self.map_files = pd.read_csv(map_csv_file)\n",
        "        self.root_dir = \"/content/dataset/exported_maps/maps/\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.map_files)\n",
        "\n",
        "    def __getitem__(self, idx, visualize=False):\n",
        "        # return the masking of that image file\n",
        "        map_file = self.map_files.iloc[idx, 0] # get the file_name\n",
        "        full_file_name = self.root_dir + map_file\n",
        "\n",
        "        mask_emp_np, mask_non_emp_np, mask_union_np, ph, pw, bev, img = masking(full_file_name, visualize)\n",
        "        mask_emp, mask_non_emp, mask_union = apply_mask(bev, mask_emp_np, mask_non_emp_np, mask_union_np, False)\n",
        "        return bev, mask_emp, mask_non_emp, mask_union, mask_emp_np, mask_non_emp_np, mask_union_np, ph, pw, img"
      ],
      "metadata": {
        "id": "IuKN_30iw4WC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BEV_JEPA"
      ],
      "metadata": {
        "id": "b3-zUHeQsexJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    \"\"\"Basic Conv → BN → GELU block\"\"\"\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.GELU(),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "\n",
        "class BEVJEPAEncoder2D(nn.Module):\n",
        "    \"\"\"\n",
        "    2D JEPA Context/Target Encoder (replaces TokenMLPEncoder)\n",
        "    - 4 CNN stages (Zhu JEPA topological equivalent)\n",
        "    - Output: BEV tokens (B, HW, C)\n",
        "    \"\"\"\n",
        "    def __init__(self, in_ch=3, base_dim=64):\n",
        "        super().__init__()\n",
        "\n",
        "        C = base_dim\n",
        "\n",
        "        # -------- Stage 1 --------\n",
        "        self.s1 = nn.Sequential(\n",
        "            ConvBlock(in_ch, C),\n",
        "            ConvBlock(C, C),\n",
        "            ConvBlock(C, C),\n",
        "        )\n",
        "\n",
        "        # -------- Stage 2 --------\n",
        "        self.s2 = nn.Sequential(\n",
        "            nn.Conv2d(C, 2*C, kernel_size=3, stride=2, padding=1),\n",
        "            nn.GELU(),\n",
        "            ConvBlock(2*C, 2*C),\n",
        "            ConvBlock(2*C, 2*C),\n",
        "        )\n",
        "\n",
        "        # -------- Stage 3 --------\n",
        "        self.s3 = nn.Sequential(\n",
        "            nn.Conv2d(2*C, 4*C, kernel_size=3, stride=2, padding=1),\n",
        "            nn.GELU(),\n",
        "            ConvBlock(4*C, 4*C),\n",
        "            ConvBlock(4*C, 4*C),\n",
        "        )\n",
        "\n",
        "        # -------- Stage 4 --------\n",
        "        self.s4 = nn.Sequential(\n",
        "            nn.Conv2d(4*C, 8*C, kernel_size=3, stride=2, padding=1),\n",
        "            nn.GELU(),\n",
        "            ConvBlock(8*C, 8*C),\n",
        "            ConvBlock(8*C, 8*C),\n",
        "        )\n",
        "\n",
        "        self.out_dim = 8 * C\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: (B, C, H, W)\n",
        "        returns:\n",
        "            tokens: (B, HW, C_out)\n",
        "            (H', W')\n",
        "        \"\"\"\n",
        "        x = self.s1(x)\n",
        "        x = self.s2(x)\n",
        "        x = self.s3(x)\n",
        "        x = self.s4(x)\n",
        "\n",
        "        B, C, H, W = x.shape\n",
        "\n",
        "        tokens = x.flatten(2).transpose(1, 2)   # (B, H * W, C_out)\n",
        "\n",
        "        return tokens, (H, W)"
      ],
      "metadata": {
        "id": "ybREdD3fsaZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spatial Predictor"
      ],
      "metadata": {
        "id": "GzuES43nshBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SpatialPredictorCNN(nn.Module):\n",
        "    \"\"\"Predict token embeddings from token grid\"\"\"\n",
        "    def __init__(self, embed_dim=128):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(embed_dim, embed_dim, kernel_size=3, padding=1),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(embed_dim, embed_dim, kernel_size=3, padding=1),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(embed_dim, embed_dim, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, z_tokens, h, w):\n",
        "        B, N, D = z_tokens.shape\n",
        "        x = z_tokens.transpose(1, 2).reshape(B, D, h, w)\n",
        "        x = self.conv(x)\n",
        "        x = x.reshape(B, D, h * w).transpose(1, 2)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "vsNlrO7asknd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## JEPA-Tier 1 Primative Layers"
      ],
      "metadata": {
        "id": "rjcg060esm7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# from utils.mask import random_token_mask\n",
        "# from utils.losses import jepa_masked_mse, vicreg_loss, drift_loss\n",
        "# from utils.ema_buffer import ema_update, init_target_from_online, LatentBuffer\n",
        "# from .mask_jepa import BEVJEPAEncoder2D  # CNN encoder\n",
        "# from .spatial_predictor import SpatialPredictorCNN\n",
        "\n",
        "\n",
        "MASK_RATIO = 0.15\n",
        "VICREG_WEIGHT = 0.1\n",
        "DRIFT_WEIGHT = 0.05\n",
        "JEPA_WEIGHT = 1.0\n",
        "EMA_DECAY = 0.99\n",
        "\n",
        "\n",
        "class PrimitiveLayer(nn.Module):\n",
        "    def __init__(self, embed_dim=128, ema_decay=EMA_DECAY):\n",
        "        super().__init__()\n",
        "\n",
        "        self.context_encoder = BEVJEPAEncoder2D(in_ch=3, base_dim=embed_dim // 8)\n",
        "\n",
        "        self.target_encoder = BEVJEPAEncoder2D(in_ch=3, base_dim=embed_dim // 8)\n",
        "        init_target_from_online(self.context_encoder, self.target_encoder)\n",
        "\n",
        "        D = self.context_encoder.out_dim\n",
        "        self.predictor = SpatialPredictorCNN(embed_dim=D)\n",
        "\n",
        "        # ✅ Zhu-style tokens (learnable)\n",
        "        self.mask_token  = nn.Parameter(torch.zeros(1, D))\n",
        "        self.empty_token = nn.Parameter(torch.zeros(1, D))\n",
        "\n",
        "        self.ema_decay = ema_decay\n",
        "        self.buffer = LatentBuffer(embed_dim=D, ema_decay=ema_decay)\n",
        "\n",
        "    def _inject_tokens_context(self, z_c_raw, mask_empty_lat, mask_any_lat):\n",
        "      z = z_c_raw.clone()\n",
        "      B, HW, D = z.shape\n",
        "\n",
        "      # Flatten to (B, HW)\n",
        "      mask_any_lat   = mask_any_lat.reshape(B, HW).bool()\n",
        "      mask_empty_lat = mask_empty_lat.reshape(B, HW).bool()\n",
        "\n",
        "      # --- Inject masked tokens ---\n",
        "      if mask_any_lat.any():\n",
        "          num_any = int(mask_any_lat.sum().item())    # correct count\n",
        "          z[mask_any_lat] = self.mask_token.expand(num_any, -1)\n",
        "\n",
        "      if mask_empty_lat.any():\n",
        "          num_empty = int(mask_empty_lat.sum().item())\n",
        "          z[mask_empty_lat] = self.empty_token.expand(num_empty, -1)\n",
        "\n",
        "      return z\n",
        "\n",
        "\n",
        "    def _inject_tokens_target(self, z_t_raw, mask_empty_lat):\n",
        "      z = z_t_raw.clone()\n",
        "      B, HW, D = z.shape\n",
        "\n",
        "      # --- Flatten and convert to bool ---\n",
        "      mask_empty_lat = mask_empty_lat.reshape(B, HW).bool()\n",
        "\n",
        "      # --- Inject empty tokens ---\n",
        "      if mask_empty_lat.any():\n",
        "          num_empty = int(mask_empty_lat.sum().item())\n",
        "          z[mask_empty_lat] = self.empty_token.expand(num_empty, -1)\n",
        "\n",
        "      return z\n",
        "\n",
        "    def forward(self, masked_img, unmasked_img,\n",
        "            mask_empty_lat, mask_non_lat, mask_any_lat):\n",
        "      \"\"\"\n",
        "      masked_img:   (B,3,H,W)\n",
        "      unmasked_img: (B,3,H,W)\n",
        "\n",
        "      mask_*_lat: (B, Hc*Wc)  masks already resized to latent grid\n",
        "      \"\"\"\n",
        "\n",
        "      # 1) Context encoder\n",
        "      z_c_raw, (Hc, Wc) = self.context_encoder(masked_img)   # (B, Hc*Wc, D)\n",
        "\n",
        "      # 2) Insert tokens for context\n",
        "      z_c = self._inject_tokens_context(z_c_raw, mask_empty_lat, mask_any_lat)\n",
        "\n",
        "      # 3) Target encoder\n",
        "      z_t_raw, _ = self.target_encoder(unmasked_img)        # (B, Hc*Wc, D)\n",
        "\n",
        "      # 4) Insert empty tokens for target\n",
        "      z_t = self._inject_tokens_target(z_t_raw, mask_empty_lat)\n",
        "\n",
        "      # 5) Predictor uses true (Hc,Wc)\n",
        "      s_c = self.predictor(z_c, Hc, Wc)\n",
        "\n",
        "      return z_c, s_c, z_t\n",
        "\n"
      ],
      "metadata": {
        "id": "qJ7O2AvwsrZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------\n",
        "#              ATTACH LOSS METHOD TO THE LAYER\n",
        "# -------------------------------------------------------------\n",
        "def compute_jepa_loss(    s_c, s_t,\n",
        "                          z_c,\n",
        "                          mask_empty,\n",
        "                          mask_nonempty,\n",
        "                          alpha0=0.25,\n",
        "                          alpha1=0.75,\n",
        "                          beta1=1.0,\n",
        "                          beta2=1.0,\n",
        "                          lambda_jepa=1.0,\n",
        "                          lambda_reg=1.0,\n",
        "                          gamma=1.0):\n",
        "\n",
        "        return jepa_loss(\n",
        "            s_c=s_c,\n",
        "            s_t=s_t,\n",
        "            z_c=z_c,\n",
        "            mask_empty=mask_empty, # B, N\n",
        "            mask_nonempty=mask_nonempty, # B, N\n",
        "            alpha0=alpha0,\n",
        "            alpha1=alpha1,\n",
        "            beta1=beta1,\n",
        "            beta2=beta2,\n",
        "            lambda_jepa=lambda_jepa,\n",
        "            lambda_reg=lambda_reg,\n",
        "            gamma=gamma\n",
        "        )\n"
      ],
      "metadata": {
        "id": "0UHquhC0k-c4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train & Test JEPA-Tier 1 Work (FORWARD TEST-DONE)"
      ],
      "metadata": {
        "id": "dJBS30b4s3b7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "map_ds = MapDataset(map_csv_file=\"/content/map_files.csv\")\n",
        "dataloader = DataLoader(map_ds, batch_size=8, num_workers=2, pin_memory=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "Qnh-3myixCL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "primitive_layer = PrimitiveLayer(embed_dim=128).to(device)\n",
        "optimizer = torch.optim.Adam(primitive_layer.parameters(), lr=1e-4)"
      ],
      "metadata": {
        "id": "cEIzjuSNMH9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect & Test The Dataset Object\n",
        "i=100\n",
        "bev = map_ds[i][0]             # B x C x H x W\n",
        "bme = map_ds[i][1]             # B x C x H x W\n",
        "bmne = map_ds[i][2]            # B x C x H x W\n",
        "bma = map_ds[i][3]             # B x C x H x W\n",
        "mask_emp_np = map_ds[i][4]     # B x (ph x pw) = 32 x 32\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "93ygKRwiLEO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training with Commet-ML"
      ],
      "metadata": {
        "id": "yCUXJmVvtcmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install comet_ml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "kK3ZcuPHtheE",
        "outputId": "15a5a64c-51ec-4544-a9a4-9f10607596a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting comet_ml\n",
            "  Downloading comet_ml-3.54.1-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting dulwich!=0.20.33,>=0.20.6 (from comet_ml)\n",
            "  Downloading dulwich-0.24.10-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
            "Collecting everett<3.2.0,>=1.0.1 (from everett[ini]<3.2.0,>=1.0.1->comet_ml)\n",
            "  Downloading everett-3.1.0-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: jsonschema!=3.1.0,>=2.6.0 in /usr/local/lib/python3.12/dist-packages (from comet_ml) (4.25.1)\n",
            "Requirement already satisfied: psutil>=5.6.3 in /usr/local/lib/python3.12/dist-packages (from comet_ml) (5.9.5)\n",
            "Collecting python-box<7.0.0 (from comet_ml)\n",
            "  Downloading python_box-6.1.0-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: requests-toolbelt>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from comet_ml) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.12/dist-packages (from comet_ml) (2.32.4)\n",
            "Requirement already satisfied: rich>=13.3.2 in /usr/local/lib/python3.12/dist-packages (from comet_ml) (13.9.4)\n",
            "Requirement already satisfied: semantic-version>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from comet_ml) (2.10.0)\n",
            "Requirement already satisfied: sentry-sdk>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from comet_ml) (2.45.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from comet_ml) (75.2.0)\n",
            "Requirement already satisfied: simplejson in /usr/local/lib/python3.12/dist-packages (from comet_ml) (3.20.2)\n",
            "Requirement already satisfied: urllib3>=1.26.8 in /usr/local/lib/python3.12/dist-packages (from comet_ml) (2.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.2 in /usr/local/lib/python3.12/dist-packages (from comet_ml) (2.0.1)\n",
            "Requirement already satisfied: wurlitzer>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from comet_ml) (3.1.1)\n",
            "Collecting configobj (from everett[ini]<3.2.0,>=1.0.1->comet_ml)\n",
            "  Downloading configobj-5.0.9-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (0.29.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.18.4->comet_ml) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.18.4->comet_ml) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.18.4->comet_ml) (2025.11.12)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.3.2->comet_ml) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.3.2->comet_ml) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=13.3.2->comet_ml) (0.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from referencing>=0.28.4->jsonschema!=3.1.0,>=2.6.0->comet_ml) (4.15.0)\n",
            "Downloading comet_ml-3.54.1-py3-none-any.whl (775 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m775.1/775.1 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dulwich-0.24.10-cp312-cp312-manylinux_2_28_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading everett-3.1.0-py2.py3-none-any.whl (35 kB)\n",
            "Downloading python_box-6.1.0-py3-none-any.whl (27 kB)\n",
            "Downloading configobj-5.0.9-py2.py3-none-any.whl (35 kB)\n",
            "Installing collected packages: everett, python-box, dulwich, configobj, comet_ml\n",
            "  Attempting uninstall: python-box\n",
            "    Found existing installation: python-box 7.3.2\n",
            "    Uninstalling python-box-7.3.2:\n",
            "      Successfully uninstalled python-box-7.3.2\n",
            "Successfully installed comet_ml-3.54.1 configobj-5.0.9 dulwich-0.24.10 everett-3.1.0 python-box-6.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from comet_ml import Experiment\n",
        "experiment = Experiment(\n",
        "    api_key=\"YOUR_API_KEY\",\n",
        "    project_name=\"jepa-training\",\n",
        "    workspace=\"YOUR_WORKSPACE\",\n",
        ")\n",
        "\n",
        "experiment.set_name(\"JEPA-PrimitiveLayer-v1\")\n",
        "experiment.add_tag(\"jepa\")\n",
        "experiment.add_tag(\"primitive-layer\")\n",
        "experiment.add_tag(\"masked-tokens\")"
      ],
      "metadata": {
        "id": "s19iEmwRtuQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "experiment.log_parameters({\n",
        "    \"lambda_jepa\": 1.0,\n",
        "    \"lambda_reg\": 10.0,\n",
        "    \"alpha0\": 0.25,\n",
        "    \"alpha1\": 0.75,\n",
        "    \"beta1\": 1.0,\n",
        "    \"beta2\": 1.0,\n",
        "    \"gamma\": 1.0,\n",
        "    \"lr\": optimizer.param_groups[0][\"lr\"],\n",
        "})"
      ],
      "metadata": {
        "id": "wYkBr5LLtwDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "\n",
        "lambda_jepa = 1.0\n",
        "lambda_reg  = 10.0\n",
        "alpha0 = 0.25\n",
        "alpha1 = 0.75\n",
        "beta1  = 1.0\n",
        "beta2  = 1.0\n",
        "gamma  = 1.0\n",
        "\n",
        "loss_history = {\n",
        "    \"total\": [],\n",
        "    \"jepa\": [],\n",
        "    \"empty\": [],\n",
        "    \"nonempty\": [],\n",
        "    \"reg\": []\n",
        "}\n",
        "\n",
        "primitive_layer.train()\n",
        "\n",
        "for batch in tqdm(dataloader, desc=\"Training JEPA\"):\n",
        "\n",
        "    # ----------------------------------------------------------\n",
        "    # Unpack batch\n",
        "    # ----------------------------------------------------------\n",
        "    (\n",
        "        bev,\n",
        "        mask_emp,\n",
        "        mask_non_emp,\n",
        "        mask_union,\n",
        "        mask_emp_np,\n",
        "        mask_non_emp_np,\n",
        "        mask_union_np,\n",
        "        ph,\n",
        "        pw,\n",
        "        img\n",
        "    ) = batch\n",
        "\n",
        "    B = bev.shape[0]\n",
        "\n",
        "    # ----------------------------------------------------------\n",
        "    # Move everything to device BEFORE forward()\n",
        "    # ----------------------------------------------------------\n",
        "    bev = bev.squeeze(1).to(device)\n",
        "\n",
        "    mask_emp      = mask_emp.to(device)\n",
        "    mask_non_emp  = mask_non_emp.to(device)\n",
        "    mask_union    = mask_union.to(device)\n",
        "\n",
        "    mask_emp_np      = mask_emp_np.to(device)\n",
        "    mask_non_emp_np  = mask_non_emp_np.to(device)\n",
        "    mask_union_np    = mask_union_np.to(device)\n",
        "\n",
        "    # ----------------------------------------------------------\n",
        "    # Build 32×32 grid masks\n",
        "    # ----------------------------------------------------------\n",
        "    mask_emp_grid  = mask_emp_np.view(B, 1, 32, 32).float()\n",
        "    mask_non_grid  = mask_non_emp_np.view(B, 1, 32, 32).float()\n",
        "    mask_any_grid  = mask_union_np.view(B, 1, 32, 32).float()\n",
        "\n",
        "    # ----------------------------------------------------------\n",
        "    # Upsample to 64×64 for latent injection\n",
        "    # ----------------------------------------------------------\n",
        "    mask_emp_up  = F.interpolate(mask_emp_grid, size=(64, 64), mode=\"nearest\")\n",
        "    mask_non_up  = F.interpolate(mask_non_grid, size=(64, 64), mode=\"nearest\")\n",
        "    mask_any_up  = F.interpolate(mask_any_grid, size=(64, 64), mode=\"nearest\")\n",
        "\n",
        "    # ----------------------------------------------------------\n",
        "    # Forward through primitive layer\n",
        "    # ----------------------------------------------------------\n",
        "    z_c, s_c, z_t = primitive_layer.forward(\n",
        "        mask_emp.squeeze(1).to(device),\n",
        "        mask_non_emp.squeeze(1).to(device),\n",
        "        mask_emp_up,\n",
        "        mask_non_up,\n",
        "        mask_any_up\n",
        "    )\n",
        "\n",
        "    # ----------------------------------------------------------\n",
        "    # Normalize latent vectors\n",
        "    # ----------------------------------------------------------\n",
        "    z_c_norm = F.normalize(z_c, dim=-1)\n",
        "    s_c_norm = F.normalize(s_c, dim=-1)\n",
        "    z_t_norm = F.normalize(z_t, dim=-1)\n",
        "\n",
        "    # ----------------------------------------------------------\n",
        "    # Flatten 64×64 masks → reduce to per-batch mask flags\n",
        "    # ----------------------------------------------------------\n",
        "    mask_non_flat = mask_non_up.bool()  # (B, 1, 64, 64) → bool\n",
        "    mask_emp_flat = mask_emp_up.bool()\n",
        "\n",
        "    # reduce across all channels except batch\n",
        "    mask_non_flat = mask_non_flat.view(B, -1)\n",
        "    mask_emp_flat = mask_emp_flat.view(B, -1)\n",
        "\n",
        "    # ----------------------------------------------------------\n",
        "    # Compute JEPA loss\n",
        "    # ----------------------------------------------------------\n",
        "    losses = compute_jepa_loss(\n",
        "        s_c=s_c_norm,\n",
        "        s_t=z_t_norm,\n",
        "        z_c=z_c_norm,\n",
        "        mask_empty=mask_emp_flat,\n",
        "        mask_nonempty=mask_non_flat,\n",
        "        alpha0=alpha0,\n",
        "        alpha1=alpha1,\n",
        "        beta1=beta1,\n",
        "        beta2=beta2,\n",
        "        lambda_jepa=lambda_jepa,\n",
        "        lambda_reg=lambda_reg,\n",
        "        gamma=gamma,\n",
        "    )\n",
        "\n",
        "    experiment.log_metric(\"loss_total\", losses[\"loss_total\"].item())\n",
        "    experiment.log_metric(\"loss_jepa\", losses[\"loss_jepa\"].item())\n",
        "    experiment.log_metric(\"loss_empty\", losses[\"loss_P_empty\"].item())\n",
        "    experiment.log_metric(\"loss_nonempty\", losses[\"loss_Q_nonempty\"].item())\n",
        "    experiment.log_metric(\"loss_reg\", losses[\"loss_reg\"].item())\n",
        "\n",
        "    # ----------------------------------------------------------\n",
        "    # RECORD LOSSES LOCALY\n",
        "    # ----------------------------------------------------------\n",
        "    loss_history[\"total\"].append(losses[\"loss_total\"].item())\n",
        "    loss_history[\"jepa\"].append(losses[\"loss_jepa\"].item())\n",
        "    loss_history[\"empty\"].append(losses[\"loss_P_empty\"].item())\n",
        "    loss_history[\"nonempty\"].append(losses[\"loss_Q_nonempty\"].item())\n",
        "    loss_history[\"reg\"].append(losses[\"loss_reg\"].item())\n",
        "\n",
        "    # ----------------------------------------------------------\n",
        "    # Backprop + optimization\n",
        "    # ----------------------------------------------------------\n",
        "    optimizer.zero_grad()\n",
        "    loss = losses[\"loss_total\"]\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # ----------------------------------------------------------\n",
        "    # EMA update\n",
        "    # ----------------------------------------------------------\n",
        "    ema_update(\n",
        "        primitive_layer.context_encoder,\n",
        "        primitive_layer.target_encoder,\n",
        "        primitive_layer.ema_decay\n",
        "    )\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "rLilyVXDyGDE",
        "outputId": "97de97d9-8c22-472d-c112-161e8e94a6df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training JEPA:   0%|          | 1/3971 [00:49<54:34:53, 49.49s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1800174171.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;31m# IMPORTANT: primitive_layer returns a tuple → no .to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;31m# ----------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     z_c, s_c, z_t = primitive_layer.forward(\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mmask_emp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m       \u001b[0;31m# (B, H*W)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mmask_non_emp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-106596122.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, masked_img, unmasked_img, mask_empty_lat, mask_non_lat, mask_any_lat)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m       \u001b[0;31m# 5) Predictor uses true (Hc,Wc)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m       \u001b[0ms_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mz_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-473437082.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, z_tokens, h, w)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \"\"\"\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    541\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             )\n\u001b[0;32m--> 543\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    544\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model saving (checkpoint)\n",
        "torch.save(primitive_layer.state_dict(), \"primitive_layer.pt\")\n",
        "experiment.log_asset(\"primitive_layer.pt\")"
      ],
      "metadata": {
        "id": "Y9AgqUnAuKtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "experiment.log_metric(\"final_loss_total\", losses[\"loss_total\"].item())\n",
        "experiment.end() # END EXPERIMENT\n",
        "\n",
        "# go to this for checking the process: https://www.comet.com/YOUR_WORKSPACE/jepa-training"
      ],
      "metadata": {
        "id": "2d1eHj20uKcf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}